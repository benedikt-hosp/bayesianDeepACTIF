import os
import torch
import pandas as pd
import numpy as np

from src.dataset_classes.giw_dataset import GIWDataset
from src.dataset_classes.TuftsDataset import TuftsDataset
from src.models.FOVAL.foval_trainer import FOVALTrainer
from FeatureRankingsCreator import FeatureRankingsCreator
from src.models.FOVAL.foval_preprocessor import input_features
import warnings
from src.dataset_classes.robustVision_dataset import RobustVisionDataset

# ================ Display options
warnings.filterwarnings('ignore')
pd.set_option('display.max_columns', None)
pd.option_context('mode.use_inf_as_na', True)

# ================ Randomization seed
np.random.seed(42)
torch.manual_seed(42)

# ================ Device options
device = torch.device("mps")  # Replace 0 with the device number for your other GPU

# ================ Save folder options
model_save_dir = "models"
os.makedirs(model_save_dir, exist_ok=True)
BASE_DIR = './'
MODEL = "FOVAL"
DATASET_NAME = "ROBUSTVISION"  # "ROBUSTVISION"  "TUFTS" "GIW"


def build_paths(base_dir):
    print("BASE_DIR is ", base_dir)
    paths = {"model_save_dir": os.path.join(base_dir, "model_archive"),
             "results_dir": os.path.join(base_dir, "results"),
             "data_base": os.path.join(base_dir, "data", "input"),
             "model_path": os.path.join(base_dir, "models", MODEL, "config", MODEL),
             "config_path": os.path.join(base_dir, "models", MODEL, "config", MODEL)}

    paths["data_dir"] = os.path.join(paths["data_base"], DATASET_NAME)
    paths["results_folder_path"] = os.path.join(paths["results_dir"], MODEL, DATASET_NAME, "FeaturesRankings_Creation")
    paths["evaluation_metrics_save_path"] = os.path.join(paths["results_dir"], MODEL, DATASET_NAME)

    for path in paths.values():
        os.makedirs(os.path.dirname(path), exist_ok=True)
    return paths


if __name__ == '__main__':
    # Parameterize MODEL and DATASET folders
    paths = build_paths(BASE_DIR)

    # == == == == == == == == == == == == == == == == == == == == == == == == == == == == == == == == == == == == == == ==
    # 1. Define Dataset
    # == == == == == == == == == == == == == == == == == == == == == == == == == == == == == == == == == == == == == == ==
    datasetName = DATASET_NAME
    if DATASET_NAME == "ROBUSTVISION":
        dataset = RobustVisionDataset(data_dir=paths["data_dir"], sequence_length=10)
        dataset.load_data()
        num_repetitions = len(dataset.subject_list)
    elif DATASET_NAME == "GIW":
        dataset = GIWDataset(data_dir=paths["data_dir"], trial_name="T4_tea_making")
        dataset.load_data()
        num_repetitions = len(dataset.subject_list)
    elif DATASET_NAME == "TUFTS":
        dataset = TuftsDataset(data_dir=paths["data_dir"])
        dataset.load_data()
        num_repetitions = len(dataset.subject_list)
    else:
        print("No dataset chosen.")

    # == == == == == == == == == == == == == == == == == == == == == == == == == == == == == == == == == == == == == == ==
    # 2. Define Model
    # == == == == == == == == == == == == == == == == == == == == == == == == == == == == == == == == == == == == == == ==
    if MODEL == "FOVAL":
        trainer = FOVALTrainer(config_path=paths["config_path"], dataset=dataset, device=device,
                               feature_names=input_features, save_intermediates_every_epoch=False)
        trainer.setup(features=input_features)

    # == == == == == == == == == == == == == == == == == == == == == == == == == == == == == == == == == == == == == == ==
    # 3. Create Ranked Lists
    # ACTIF Creation: Calculate feature importance ranking for all methods collect sorted ranking list, memory usage, and computation speed
    # == == == == == == == == == == == == == == == == == == == == == == == == == == == == == == == == == == == == == == ==
    fmv = FeatureRankingsCreator(modelName=MODEL, datasetName=DATASET_NAME, dataset=dataset, trainer=trainer,
                                 paths=paths, device=device)
    fmv.process_methods()

'''

01.02.2025:
    
    # Ziel: Feature Importance Ranking of neural network

    # 1. Modelle:
    #   FOVAL
    #   XXX: 1 Datensatz
    #   YYY: 1 Datensatz

    # 2. Methoden:
    # 2.1 Modellagnostische Methoden (Blackbox-Modelle interpretieren)
    # Deeplift:     zero, random, mean baselines
    # deepactif:
    # IntGrad:
    # SHAP (KernelSHAP, TreeSHAP)
    # Ablation:
    # Permutation:
    # NEU
    # LIME / Anchors
    # LRP
    # Occlusion Sensitivity


    # 2.2 Gradienten-basierte (f√ºr neuronale Netze)
    # Vanilla Gradients (Saliency Maps) -> BesseR: Guided Backpropagation
    # Smoothgrad
    # DeepLIFT

    # 2.3 Methoden f√ºr bestimmete Architekture
    # 2.3.1 CNNs
    # GRAD-CAM / GRAD-CAM++ / Score-CAM
    # LRP
    # Deep Taylor Decomposition
    # Feature Visualization (Neuron Activation Maximization, NAM
    # Attention Maps f√ºr Vision Transformer (ViTs)

    # 2.4 LSTM/RNN Zeitreihne/NLP
    # Gradient-based Attribution (wie bei CNNs)
    # Attention Weight Visualization
    # Input Cell State Contribution (z. B. f√ºr LSTMs & GRUs)
    # Temporal Importance Ranking (z. B. mit Shapley Values auf Zeitfenstern)

    # 2.5 Methoden f√ºr Transformer (NLP & Vision)
	# Attention Rollout: Summiert mehrschichtige Attention-Gewichte f√ºr besser verst√§ndliche Heatmaps.
	# Attention Attribution (z. B. LIME auf Attention-Scores)
	# Gradient-based Methoden (Integrated Gradients, SHAP, DeepLIFT f√ºr Transformer)

    # 2.6 Regelbasierte Methoden
    # Antwortet nicht ‚ÄûWarum ist Feature X wichtig?‚Äú sondern ‚ÄûWelche Datenpunkte haben die Vorhersage beeinflusst?‚Äú
    # Counterfactual Explanations (CFEs)
    # 	‚Ä¢	Zeigt minimale √Ñnderungen an den Eingabedaten, die die Vorhersage √§ndern w√ºrden.
    # 	‚Ä¢	Beispiel: ‚ÄûWenn Einkommen +500‚Ç¨ w√§re ‚Üí Kreditzusage!‚Äú
    # 	‚Ä¢	Prototypen & Kritische Beispiele (ProtoDash, Deep Prototypes)
    # 	‚Ä¢	Zeigt typische Beispiele, die das Modell repr√§sentieren.
    # 	‚Ä¢	K-Nearest Neighbors (KNN) f√ºr Interpretierbarkeit
    # 	‚Ä¢	‚ÄûWelche Trainingsdaten sind der aktuellen Eingabe am √§hnlichsten?‚Äú
    # 	‚Ä¢	Influence Functions
    # 	‚Ä¢	Analysiert, welche Trainingsbeispiele eine Vorhersage am st√§rksten beeinflusst haben.
    # 	‚Ä¢	Adversarial Explanations
    # 	‚Ä¢	Identifiziert minimale Eingangs√§nderungen, die eine falsche Klassifikation verursachen.

    # 2.7 Uncertainty-aware Methoden (Feature Ranking mit Unsicherheitssch√§tzung)
    # üîπ Besonders f√ºr deine Arbeit relevant!
    # 	‚Ä¢	Bayesian Deep Learning (z. B. Monte Carlo Dropout)
    # 	‚Ä¢	Deep Ensembles f√ºr Unsicherheitsquantifizierung
    # 	‚Ä¢	Gaussian Processes √ºber Feature-Attributionen
    # 	‚Ä¢	Variance-based Feature Importance (Wie stabil ist das Ranking √ºber verschiedene Mini-Batches?)
    # 	‚Ä¢	Bayesian Feature Ranking Updating (Dynamische Anpassung des Rankings √ºber Zeit)

    # 3. Vergleiche:
    # F√ºr jedes Modell, jeden Datensatz, jede Methode, mit allen Sensitivity Parameter, mit jeder ACTIF Variante
    # Modelle: 3
    # Datens√§tze: 3
    # Methode: 10
    # Actif Varianten: 4
    # Sensitivit√§tsparameter: bis zu 3 (gesamt 60 parameterisierte Methoden)
    
    
    # TODO Suggestions for Improvement:
    # 	1. Include additional baselines like LIME and LRP to provide a more comprehensive benchmark.
    # 	2. Expand dataset diversity by including datasets from different sources but for the same task
    #    	(e.g., another biometric dataset)
    #          + TUFTS
    #          + GIW
    #       and exploring domains like finance, speech processing, and industrial monitoring.
    #           # ?
    # 	3. Test additional architectures to show DeepACTIF‚Äôs flexibility beyond LSTMs.
    #       - TCNs
    #       - Transformers,
    # 	4. Provide qualitative explainability comparisons by visualizing feature importance rankings and checking consistency across different methods.
    # 	5. Analyze robustness to model and data variations, including sensitivity to LSTM size, dataset noise, and sequence length.
    # 	6. Investigate potential bias in INV by checking whether penalizing high-variability features removes important signals.

    # Auswertung: F√ºr jede Methode mean √ºber alle Datens√§tze und innerhalb Datensatz vergleichen
    #   - Memory Consumption
    #   - Computing Time
    #   - Performance     
     '''
