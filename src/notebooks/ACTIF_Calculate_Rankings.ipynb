{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Importance Calculation using Activation Data\n",
    "\n",
    "This notebook provides various methods to calculate feature importances based on activation data from neural networks. The methods include different statistical approaches such as mean activation, mean multiplied by standard deviation, and robust methods that penalize high or low variability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 40 + 2\n",
    "selected_features = [\n",
    "    'World_Gaze_Direction_R_X', 'World_Gaze_Direction_R_Y', 'World_Gaze_Direction_R_Z',\n",
    "    'World_Gaze_Direction_L_X', 'World_Gaze_Direction_L_Y', 'World_Gaze_Direction_L_Z',\n",
    "    'World_Gaze_Origin_R_X', 'World_Gaze_Origin_R_Z', 'World_Gaze_Origin_L_X', \n",
    "    'World_Gaze_Origin_L_Z', 'Vergence_Angle', 'Vergence_Depth', 'Normalized_Depth',\n",
    "    'Directional_Magnitude_R', 'Directional_Magnitude_L', 'Cosine_Angles', 'Gaze_Point_Distance',\n",
    "    'Normalized_Vergence_Angle', 'Delta_Gaze_X', 'Delta_Gaze_Y', 'Delta_Gaze_Z',\n",
    "    'Rolling_Mean_Normalized_Depth', 'Gaze_Vector_Angle', 'Gaze_Point_Depth_Difference',\n",
    "    'Relative_Change_Vergence_Angle', 'Ratio_Directional_Magnitude', 'Ratio_Delta_Gaze_XY',\n",
    "    'Ratio_World_Gaze_Direction_X', 'Ratio_World_Gaze_Direction_Y', 'Ratio_World_Gaze_Direction_Z',\n",
    "    'Interaction_Normalized_Depth_Vergence_Angle', 'Lag_1_Normalized_Depth', 'Diff_Normalized_Depth',\n",
    "    'Directional_Magnitude_Ratio', 'Gaze_Direction_X_Ratio', 'Gaze_Direction_Y_Ratio', \n",
    "    'Gaze_Direction_Z_Ratio', 'Angular_Difference_X', 'Depth_Angle_Interaction', \n",
    "    'Gaze_Point_Euclidean_Distance', 'Gaze_Direction_Angle', 'Velocity_Gaze_Direction_R_X', \n",
    "    'Acceleration_Gaze_Direction_R_X', 'Velocity_Gaze_Direction_R_Y', 'Acceleration_Gaze_Direction_R_Y', \n",
    "    'Velocity_Gaze_Direction_R_Z', 'Acceleration_Gaze_Direction_R_Z', 'Velocity_Gaze_Direction_L_X', \n",
    "    'Acceleration_Gaze_Direction_L_X', 'Velocity_Gaze_Direction_L_Y', 'Acceleration_Gaze_Direction_L_Y', \n",
    "    'Velocity_Gaze_Direction_L_Z', 'Acceleration_Gaze_Direction_L_Z', \n",
    "    'Angular_Difference_Gaze_Directions'\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_activation_data(user_folder):\n",
    "    activation_file_path = os.path.join(user_folder, 'intermediates_activations.npy')\n",
    "    if os.path.exists(activation_file_path):\n",
    "        try:\n",
    "            activations_data = np.load(activation_file_path, allow_pickle=True).item()\n",
    "            layer_activation = list(activations_data.values())[0]\n",
    "            if isinstance(layer_activation, np.ndarray) and layer_activation.size > 0:\n",
    "                if layer_activation.ndim > 2:\n",
    "                    layer_activation = np.mean(layer_activation, axis=1)\n",
    "                return layer_activation\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading activation data from {activation_file_path}: {e}\")\n",
    "    else:\n",
    "        print(f\"Activation file does not exist at {activation_file_path}\")\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_actif_mean(activation):\n",
    "    activation_abs = np.abs(activation)\n",
    "    mean_activation = np.mean(activation_abs, axis=0)\n",
    "    std_activation = np.std(activation_abs, axis=0)\n",
    "    return mean_activation, mean_activation, std_activation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_actif_meanstddev(activation):\n",
    "    activation_abs = np.abs(activation)\n",
    "    mean_activation = np.mean(activation_abs, axis=0)\n",
    "    std_activation = np.std(activation_abs, axis=0)\n",
    "    weighted_importance = mean_activation * std_activation\n",
    "    return weighted_importance, mean_activation, std_activation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_actif_weighted_mean(activation):\n",
    "    activation_abs = np.abs(activation)\n",
    "    mean_activation = np.mean(activation_abs, axis=0)\n",
    "    normalized_mean = (mean_activation - np.min(mean_activation)) / (\n",
    "        np.max(mean_activation) - np.min(mean_activation))\n",
    "    std_activation = np.std(activation_abs, axis=0)\n",
    "    normalized_std = (std_activation - np.min(std_activation)) / (np.max(std_activation) - np.min(std_activation))\n",
    "    adjusted_importance = (normalized_mean + normalized_std) / 2\n",
    "    return adjusted_importance, mean_activation, std_activation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_actif_inverted_weighted_mean(activation):\n",
    "    activation_abs = np.abs(activation)\n",
    "    mean_activation = np.mean(activation_abs, axis=0)\n",
    "    normalized_mean = (mean_activation - np.min(mean_activation)) / (\n",
    "        np.max(mean_activation) - np.min(mean_activation))\n",
    "    std_activation = np.std(activation_abs, axis=0)\n",
    "    normalized_std = (std_activation - np.min(std_activation)) / (np.max(std_activation) - np.min(std_activation))\n",
    "    inverse_normalized_std = 1 - normalized_std\n",
    "    adjusted_importance = (normalized_mean + inverse_normalized_std) / 2\n",
    "    return adjusted_importance, mean_activation, std_activation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_actif_robust(activations, epsilon=0.01, min_std_threshold=0.01):\n",
    "    activation_abs = np.abs(activations)\n",
    "    mean_activation = np.mean(activation_abs, axis=0)\n",
    "    std_activation = np.std(activation_abs, axis=0)\n",
    "    normalized_mean = (mean_activation - np.min(mean_activation)) / (\n",
    "        np.max(mean_activation) - np.min(mean_activation) + epsilon)\n",
    "    transformed_std = np.exp(-std_activation / min_std_threshold)\n",
    "    adjusted_importance = normalized_mean * (1 - transformed_std)\n",
    "    return adjusted_importance, mean_activation, std_activation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_actif_robust_penHigh(activations, epsilon=0.01, min_std_threshold=0.01):\n",
    "    activation_abs = np.abs(activations)\n",
    "    mean_activation = np.mean(activation_abs, axis=0)\n",
    "    std_activation = np.std(activation_abs, axis=0)\n",
    "    normalized_mean = (mean_activation - np.min(mean_activation)) / (\n",
    "        np.max(mean_activation) - np.min(mean_activation) + epsilon)\n",
    "    transformed_std = np.exp(-std_activation / min_std_threshold)\n",
    "    adjusted_importance = normalized_mean * transformed_std\n",
    "    return adjusted_importance, mean_activation, std_activation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def actif_general_all_subjects(subject_folders, calculation_function, layer_index=0):\n",
    "    \"\"\"\n",
    "    Generalized function to calculate and combine feature importances across multiple subjects and layers.\n",
    "\n",
    "    Parameters:\n",
    "    - subject_folders (List[str]): List of paths to the user folders containing activation data.\n",
    "    - calculation_function (callable): A function that takes in activation data and returns feature importance.\n",
    "    - layer_index (int): Index of the layer to process (e.g., 0 or 1).\n",
    "\n",
    "    Returns:\n",
    "    - List[dict]: List of dictionaries containing feature names and their corresponding combined attributions.\n",
    "    \"\"\"\n",
    "    selected_features2 = [value for value in selected_features if value not in ('SubjectID', 'Gt_Depth')]\n",
    "    \n",
    "    # Initialize lists to collect combined results\n",
    "    all_mean_importances = []\n",
    "\n",
    "    for user_folder in subject_folders:\n",
    "        activation_file_path = os.path.join(user_folder, 'intermediates_activations.npy')\n",
    "        \n",
    "        if os.path.exists(activation_file_path):\n",
    "            try:\n",
    "                activations_data = np.load(activation_file_path, allow_pickle=True).item()\n",
    "                layer_activation = list(activations_data.values())[layer_index]  # Select the specific layer\n",
    "                \n",
    "                if isinstance(layer_activation, np.ndarray) and layer_activation.size > 0:\n",
    "                    if layer_activation.ndim > 2:\n",
    "                        layer_activation = np.mean(layer_activation, axis=1)\n",
    "                    \n",
    "                    importance, _, _ = calculation_function(layer_activation)\n",
    "                    all_mean_importances.append(importance)\n",
    "                else:\n",
    "                    print(f\"No valid activation data for layer {layer_index} in {user_folder}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {activation_file_path}: {e}\")\n",
    "        else:\n",
    "            print(f\"Activation file does not exist at {activation_file_path}\")\n",
    "\n",
    "    if all_mean_importances:\n",
    "        # Calculate the average importance across all subjects\n",
    "        combined_importances = np.mean(np.array(all_mean_importances), axis=0)\n",
    "\n",
    "        # Sort features based on combined importances\n",
    "        sorted_indices = np.argsort(-combined_importances)\n",
    "        sorted_features = np.array(selected_features2)[sorted_indices]\n",
    "        sorted_combined_importances = combined_importances[sorted_indices]\n",
    "\n",
    "        # Prepare results as a list of dictionaries\n",
    "        results = [{'feature': feature, 'attribution': sorted_combined_importances[i]} for i, feature in\n",
    "                   enumerate(sorted_features)]\n",
    "        return results\n",
    "    else:\n",
    "        return []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for method 'mean' successfully saved to feature_importance_mean.csv\n",
      "Results for method 'mean_stddev' successfully saved to feature_importance_mean_stddev.csv\n",
      "Results for method 'weighted_mean' successfully saved to feature_importance_weighted_mean.csv\n",
      "Results for method 'inverted_weighted_mean' successfully saved to feature_importance_inverted_weighted_mean.csv\n",
      "Results for method 'robust' successfully saved to feature_importance_robust.csv\n",
      "Results for method 'robust_penHigh' successfully saved to feature_importance_robust_penHigh.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def compile_results_with_layers_as_columns(subject_folders):\n",
    "    # Define the methods and corresponding functions\n",
    "    methods = {\n",
    "        'mean': calculate_actif_mean,\n",
    "        'mean_stddev': calculate_actif_meanstddev,\n",
    "        'weighted_mean': calculate_actif_weighted_mean,\n",
    "        'inverted_weighted_mean': calculate_actif_inverted_weighted_mean,\n",
    "        'robust': calculate_actif_robust,\n",
    "        'robust_penHigh': calculate_actif_robust_penHigh\n",
    "    }\n",
    "\n",
    "    # Loop through each method\n",
    "    for method_name, method_function in methods.items():\n",
    "        results = {}\n",
    "\n",
    "        # Process for both layers (0 and 1)\n",
    "        for layer_index in [0, 1]:\n",
    "            result = actif_general_all_subjects(subject_folders, method_function, layer_index=layer_index)\n",
    "            for item in result:\n",
    "                feature = item['feature']\n",
    "                if feature not in results:\n",
    "                    results[feature] = {}\n",
    "                results[feature][f'Layer {layer_index}'] = item['attribution']\n",
    "\n",
    "        # Convert the results dictionary to a DataFrame\n",
    "        df = pd.DataFrame.from_dict(results, orient='index').reset_index()\n",
    "        df.rename(columns={'index': 'Feature'}, inplace=True)\n",
    "\n",
    "        # Export DataFrame to CSV\n",
    "        output_file = f\"Rankings/feature_importance_{method_name}.csv\"\n",
    "        df.to_csv(output_file, index=False)\n",
    "\n",
    "        print(f\"Results for method '{method_name}' successfully saved to {output_file}\")\n",
    "\n",
    "# Example usage\n",
    "user_folders = 'D:\\\\git\\\\Results_FOVAL'\n",
    "# List all subject folders in the main directory\n",
    "subject_folders = [os.path.join(user_folders, name) for name in os.listdir(user_folders) if os.path.isdir(os.path.join(user_folders, name))]\n",
    "\n",
    "compile_results_with_layers_as_columns(subject_folders)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Your selected features list ...\n",
    "\n",
    "def calculate_weighted_importance_product(activation):\n",
    "    mean_activation = np.mean(activation, axis=0)\n",
    "    std_activation = np.std(activation, axis=0)\n",
    "    weighted_importance = mean_activation * std_activation\n",
    "    return weighted_importance, mean_activation, std_activation\n",
    "\n",
    "def calculate_adjusted_importance(activation):\n",
    "    mean_activation = np.mean(activation, axis=0)\n",
    "    normalized_mean = (mean_activation - np.min(mean_activation)) / (np.max(mean_activation) - np.min(mean_activation) + 1e-8)\n",
    "    std_activation = np.std(activation, axis=0)\n",
    "    normalized_std = (std_activation - np.min(std_activation)) / (np.max(std_activation) - np.min(std_activation) + 1e-8)\n",
    "    adjusted_importance = (normalized_mean + normalized_std) / 2\n",
    "    return adjusted_importance, mean_activation, std_activation\n",
    "\n",
    "results_dir = 'D:/git/Results_FOVAL'\n",
    "subject_folders = [f.path for f in os.scandir(results_dir) if f.is_dir()]\n",
    "\n",
    "subject_names = [os.path.basename(folder) for folder in subject_folders]\n",
    "\n",
    "# Placeholder arrays to collect metrics for all subjects\n",
    "all_mean_activations = []\n",
    "all_std_activations = []\n",
    "all_weighted_importances = []\n",
    "\n",
    "for folder in subject_folders:\n",
    "    activation_file_path = os.path.join(folder, 'intermediates_activations.npy')\n",
    "    if os.path.exists(activation_file_path):\n",
    "        try:\n",
    "            activations_data = np.load(activation_file_path, allow_pickle=True).item()\n",
    "            layer_activation = list(activations_data.values())[0]\n",
    "            \n",
    "            if isinstance(layer_activation, np.ndarray) and layer_activation.size > 0:\n",
    "                if layer_activation.ndim > 2:\n",
    "                    layer_activation = np.mean(layer_activation, axis=1)\n",
    "\n",
    "                weighted_importance, mean_activation, std_activation = calculate_adjusted_importance(layer_activation)\n",
    "                all_mean_activations.append(mean_activation)\n",
    "                all_std_activations.append(std_activation)\n",
    "                all_weighted_importances.append(weighted_importance)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {activation_file_path}: {e}\")\n",
    "\n",
    "# Ensure all lists are not empty\n",
    "if all_mean_activations and all_std_activations and all_weighted_importances:\n",
    "    mean_activations = np.mean(np.array(all_mean_activations), axis=0)\n",
    "    std_activations = np.mean(np.array(all_std_activations), axis=0)\n",
    "    weighted_importances = np.mean(np.array(all_weighted_importances), axis=0)\n",
    "\n",
    "    sorted_indices = np.argsort(weighted_importances)\n",
    "    sorted_features = np.array(selected_features)[sorted_indices]\n",
    "    sorted_mean = mean_activations[sorted_indices]\n",
    "    sorted_std = std_activations[sorted_indices]\n",
    "    sorted_weighted_mean = weighted_importances[sorted_indices]\n",
    "\n",
    "    n_features = len(sorted_features)\n",
    "    ind = np.arange(n_features)\n",
    "    width = 0.25\n",
    "\n",
    "    plt.figure(figsize=(18, 10))\n",
    "    bars1 = plt.bar(ind - width, sorted_mean, width, color='blue', label='Mean Activation')\n",
    "    bars2 = plt.bar(ind, sorted_std, width, color='red', label='Standard Deviation')\n",
    "    bars3 = plt.bar(ind + width, sorted_weighted_mean, width, color='green', label='Weighted Mean Activation')\n",
    "\n",
    "    plt.ylabel('Scores')\n",
    "    plt.title('Feature Importance Metrics Across All Subjects (First Layer)')\n",
    "    plt.xticks(ind, sorted_features, rotation=90)\n",
    "    plt.xlabel('Features')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    most_important_feature_index = np.argmin(sorted_weighted_mean)\n",
    "    most_important_feature = sorted_features[most_important_feature_index]\n",
    "    print(most_important_feature)\n",
    "else:\n",
    "    print(\"No valid activation data found.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda111",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
